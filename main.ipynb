{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/training.1600000.processed.noemoticon 2.csv', encoding='latin-1', header=None)\n",
    "df.columns = ['polarity', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#', '', text)        # Remove hashtags\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the text column\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Save the cleaned data to a new file\n",
    "clean_path = '/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/cleaned_tweets.csv'\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f'Cleaned data saved to {clean_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENISE AND REMOVING STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources if not already installed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the cleaned data\n",
    "df = pd.read_csv(clean_path)\n",
    "\n",
    "# Define a function to tokenize and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply tokenization and stop word removal\n",
    "df['tokenized_text'] = df['clean_text'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Save the tokenized data to a new file\n",
    "tokenized_path = '/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/tokenized_tweets.csv'\n",
    "df.to_csv(tokenized_path, index=False)\n",
    "print(f'Tokenized data saved to {tokenized_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMMATIZING WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define a function for lemmatization with a check for non-string types\n",
    "def lemmatize_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return ''  # or return np.nan if you prefer to keep NaN values\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Assume df is your DataFrame loaded from the tokenized CSV\n",
    "# Apply the lemmatization function\n",
    "df['lemmatized_text'] = df['tokenized_text'].apply(lemmatize_text)\n",
    "\n",
    "# Continue with your processing or saving steps\n",
    "\n",
    "\n",
    "# Save the final processed data\n",
    "final_output_path = '/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/processed_tweets.csv'\n",
    "df.to_csv(final_output_path, index=False)\n",
    "print(f'Final processed data saved to {final_output_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the processed data\n",
    "df = pd.read_csv('/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/processed_tweets.csv')\n",
    "\n",
    "# Sample 25% of the data randomly\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizes text unless it's NaN, in which case it returns an empty list.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return word_tokenize(str(text))\n",
    "\n",
    "def apply_tokenization(column):\n",
    "    \"\"\"Applies the tokenization across a pandas Series with a progress bar for visibility.\"\"\"\n",
    "    return [tokenize_text(text) for text in tqdm(column, total=len(column), desc='Tokenizing Text')]\n",
    "\n",
    "def calculate_word_frequencies(tokens_list):\n",
    "    \"\"\"Calculates and returns the frequency distribution of words in a list of tokens.\"\"\"\n",
    "    all_tokens = sum(tokens_list, [])\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "    return pd.DataFrame(freq_dist.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Tokenization step with progress bar\n",
    "df_sampled['tokens'] = apply_tokenization(df_sampled['tokenized_text'])\n",
    "\n",
    "# Separate data by sentiment\n",
    "negative_df = df_sampled[df_sampled['polarity'] == 0]\n",
    "positive_df = df_sampled[df_sampled['polarity'] == 4]\n",
    "\n",
    "# Calculate word frequencies with progress bars\n",
    "negative_freq = calculate_word_frequencies(tqdm(negative_df['tokens'], desc='Processing Negative Tokens'))\n",
    "positive_freq = calculate_word_frequencies(tqdm(positive_df['tokens'], desc='Processing Positive Tokens'))\n",
    "\n",
    "# Prepare data for comparison\n",
    "top_n = 20\n",
    "top_negative_words = negative_freq.head(top_n)\n",
    "top_positive_words = positive_freq.head(top_n)\n",
    "compare_df = pd.merge(top_negative_words, top_positive_words, on='Word', how='outer', suffixes=('_neg', '_pos'))\n",
    "compare_df.fillna(0, inplace=True)\n",
    "\n",
    "def plot_data(data_frame):\n",
    "    \"\"\"Generates and displays plots for the given data.\"\"\"\n",
    "    # Bar Plot Comparison\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    data_frame.set_index('Word').plot(kind='bar', figsize=(14, 6))\n",
    "    plt.title('Top Word Frequencies by Sentiment')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Words')\n",
    "    plt.show()\n",
    "\n",
    "    # Violin Plot for frequency distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.violinplot(data=data_frame[['Frequency_neg', 'Frequency_pos']])\n",
    "    plt.title('Distribution of Word Frequencies in Negative and Positive Sentiments')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.show()\n",
    "\n",
    "    # Heatmap of word frequencies\n",
    "    heatmap_data = data_frame[['Frequency_neg', 'Frequency_pos']].set_index(data_frame['Word'])\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "    plt.title('Heatmap of Word Frequencies')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the results with data\n",
    "plot_data(compare_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VECTORISING USING WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/processed_tweets.csv')\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_bow = vectorizer.fit_transform(df['lemmatized_text'].fillna(''))\n",
    "\n",
    "# Example of feature names (vocabulary)\n",
    "print(vectorizer.get_feature_names_out()[:10])  # Display first 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['lemmatized_text'].fillna(''))\n",
    "\n",
    "# Example of feature names\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:10])  # Display first 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0374963   0.35365114 -0.20733492  0.12490724  0.11060168 -0.52926284\n",
      " -0.01267704  0.718971   -0.22653954 -0.26553535 -0.02834426 -0.5234946\n",
      " -0.15810917  0.20502315  0.14462076 -0.20753594  0.03194011 -0.48278597\n",
      "  0.32928815 -0.53496736  0.3684711   0.11661899  0.1611437  -0.07528108\n",
      " -0.13008443  0.1123393  -0.06817615 -0.37413526 -0.14680852 -0.02311885\n",
      "  0.5484645   0.12794554  0.06130024 -0.05523879 -0.03161808  0.21044289\n",
      "  0.26324677 -0.5094701  -0.35537186 -0.65278095 -0.08233742 -0.00400269\n",
      " -0.03520166 -0.1864458   0.23173957 -0.3685568  -0.2692534  -0.12608714\n",
      "  0.38196635  0.14202121 -0.19212578  0.11606234 -0.32023388 -0.02379689\n",
      "  0.107067    0.12006015 -0.02724012  0.02789338 -0.10309459 -0.15598905\n",
      "  0.24929479  0.1583937  -0.14580435  0.2572062  -0.48486683 -0.01825565\n",
      "  0.01085354  0.03053951 -0.3917125   0.5081967  -0.29258594 -0.24253051\n",
      "  0.05439566  0.2672059   0.15558267  0.06305068  0.1428546  -0.11899947\n",
      " -0.43466514  0.04146253 -0.1702961   0.2849282  -0.17424229  0.33866867\n",
      " -0.07527     0.07577559 -0.34879452  0.5242475   0.3312672   0.24735247\n",
      "  0.37415266  0.27410498  0.0475466   0.15716363  0.3220082   0.2797344\n",
      " -0.07755253 -0.34698197  0.31128788  0.30062342]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# Tokenize the sentences\n",
    "df['tokens'] = df['lemmatized_text'].fillna('').apply(nltk.word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model \n",
    "model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Example of getting a vector for a word\n",
    "word_vector = model.wv['example']  # Replace 'example' with a relevant word from your dataset\n",
    "print(word_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FITTING AND TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/processed_tweets.csv')\n",
    "\n",
    "# Sample 10% of the data\n",
    "df_sampled = df.sample(frac=0.25, random_state=42)\n",
    "\n",
    "# Save the sampled data to a new CSV file\n",
    "df_sampled.to_csv('/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/sampled_tweets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/matzchan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111e4251363b4201813adfb7b2212689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Text:   0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm  # Import tqdm for Jupyter Notebook compatibility\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/sampled_tweets.csv')\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Check if 'tokenized_text' exists and then tokenize it with a progress bar\n",
    "if 'tokenized_text' in df.columns:\n",
    "    tqdm.pandas(desc=\"Tokenizing Text\")  # Set up tqdm with pandas\n",
    "    df['tokens'] = df['tokenized_text'].progress_apply(lambda x: word_tokenize(str(x)) if pd.notna(x) else [])\n",
    "else:\n",
    "    print(\"Error: 'tokenized_text' column does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer saved successfully to /Users/matzchan/docs/Y1S2/SC15 DSAI/lab/LAB 8/tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Convert tokens back to strings for TF-IDF vectorization\n",
    "df['text_str'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize and fit the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Adjust as necessary\n",
    "X = tfidf_vectorizer.fit_transform(df['text_str']).toarray()\n",
    "\n",
    "# Save the TF-IDF vectorizer to disk using pickle\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "    print(f\"TF-IDF vectorizer saved successfully to {os.path.abspath(file.name)}\")\n",
    "\n",
    "# Assuming 'tokens' is a list of tokenized text\n",
    "documents = df['tokens'].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = gensim.models.Word2Vec(documents, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Function to average Word2Vec vectors for a document\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "\n",
    "# Prepare feature vectors for all documents\n",
    "w2v_features = np.array([average_word_vectors(doc, word2vec_model, word2vec_model.wv.key_to_index, 100) for doc in documents])\n",
    "\n",
    "# Prepare target variable\n",
    "y = df['polarity']  # Assuming 'polarity' is your target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(w2v_features, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.71     49892\n",
      "           4       0.73      0.59      0.65     50108\n",
      "\n",
      "    accuracy                           0.69    100000\n",
      "   macro avg       0.69      0.69      0.68    100000\n",
      "weighted avg       0.69      0.69      0.68    100000\n",
      "\n",
      "Model saved successfully to naive_bayes_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "\n",
    "# Initialize and train the Naive Bayes model (GaussianNB assuming continuous features)\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Save the trained Naive Bayes model using pickle\n",
    "model_filename = 'naive_bayes_model.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(nb_model, file)\n",
    "    print(f\"Model saved successfully to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71     49892\n",
      "           4       0.72      0.60      0.66     50108\n",
      "\n",
      "    accuracy                           0.68    100000\n",
      "   macro avg       0.69      0.68      0.68    100000\n",
      "weighted avg       0.69      0.68      0.68    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize and train the Naive Bayes model (GaussianNB assuming continuous features)\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, nb_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.73      0.57        11\n",
      "           1       0.62      0.36      0.45        14\n",
      "\n",
      "    accuracy                           0.52        25\n",
      "   macro avg       0.55      0.54      0.51        25\n",
      "weighted avg       0.56      0.52      0.51        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, lr_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SVM:   0%|          | 0/100 [00:00<?, ?%/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71        17\n",
      "           1       0.38      0.38      0.38         8\n",
      "\n",
      "    accuracy                           0.60        25\n",
      "   macro avg       0.54      0.54      0.54        25\n",
      "weighted avg       0.60      0.60      0.60        25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SVM: 100%|██████████| 100/100 [00:30<00:00,  3.27%/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from sklearn.svm import SVC  # Ensure this import is present\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np  # Assuming your X_train, etc., are numpy arrays or similar\n",
    "\n",
    "# Assuming data setup for demonstration\n",
    "# X_train, X_test, y_train, y_test = train_test_split(...)\n",
    "\n",
    "# Function to simulate training progress\n",
    "def simulate_training_progress(duration):\n",
    "    pbar = tqdm(total=100, desc='Training SVM', unit='%', leave=True)\n",
    "    for i in range(100):\n",
    "        time.sleep(duration / 100)  # Sleep to simulate work\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "# Define the SVM training as a thread\n",
    "class TrainSVM(threading.Thread):\n",
    "    def __init__(self, X_train, y_train, X_test):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "\n",
    "    def run(self):\n",
    "        svm_model = SVC(kernel='linear')\n",
    "        svm_model.fit(self.X_train, self.y_train)\n",
    "        svm_predictions = svm_model.predict(self.X_test)\n",
    "        print(\"SVM Classification Report:\\n\", classification_report(y_test, svm_predictions))\n",
    "\n",
    "# Start the training and progress simulation\n",
    "X_train, X_test, y_train, y_test = np.random.rand(100, 10), np.random.rand(25, 10), np.random.randint(0, 2, 100), np.random.randint(0, 2, 25)  # Example setup\n",
    "training_thread = TrainSVM(X_train, y_train, X_test)\n",
    "training_thread.start()\n",
    "\n",
    "# Simulate training progress for an estimated duration (e.g., 30 seconds)\n",
    "simulate_training_progress(30)\n",
    "\n",
    "# Ensure the training thread has completed\n",
    "training_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64        11\n",
      "           1       0.71      0.71      0.71        14\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.68      0.68      0.68        25\n",
      "weighted avg       0.68      0.68      0.68        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)  # Assuming X_train and y_train are properly prepared\n",
    "\n",
    "# Predict and evaluate\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6922 - val_accuracy: 0.4000 - val_loss: 0.6943\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5571 - loss: 0.6919 - val_accuracy: 0.4000 - val_loss: 0.6977\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5143 - loss: 0.6916 - val_accuracy: 0.4000 - val_loss: 0.7007\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5143 - loss: 0.6947 - val_accuracy: 0.4000 - val_loss: 0.7023\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5143 - loss: 0.6911 - val_accuracy: 0.4000 - val_loss: 0.7029\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5143 - loss: 0.6891 - val_accuracy: 0.4000 - val_loss: 0.7028\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5143 - loss: 0.6940 - val_accuracy: 0.4000 - val_loss: 0.7019\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5286 - loss: 0.6950 - val_accuracy: 0.4000 - val_loss: 0.7010\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5143 - loss: 0.6937 - val_accuracy: 0.4000 - val_loss: 0.6997\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5143 - loss: 0.6942 - val_accuracy: 0.4000 - val_loss: 0.6981\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5286 - loss: 0.6914 - val_accuracy: 0.4000 - val_loss: 0.6967\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5000 - loss: 0.6911 - val_accuracy: 0.4000 - val_loss: 0.6955\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5429 - loss: 0.6907 - val_accuracy: 0.4000 - val_loss: 0.6945\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5143 - loss: 0.6928 - val_accuracy: 0.4000 - val_loss: 0.6936\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5286 - loss: 0.6939 - val_accuracy: 0.4000 - val_loss: 0.6933\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5000 - loss: 0.6934 - val_accuracy: 0.4000 - val_loss: 0.6934\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5143 - loss: 0.6931 - val_accuracy: 0.4000 - val_loss: 0.6940\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.4857 - loss: 0.6947 - val_accuracy: 0.4000 - val_loss: 0.6948\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5857 - loss: 0.6878 - val_accuracy: 0.4000 - val_loss: 0.6957\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.5286 - loss: 0.6914 - val_accuracy: 0.4000 - val_loss: 0.6968\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5143 - loss: 0.6917 - val_accuracy: 0.4000 - val_loss: 0.6976\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4857 - loss: 0.6912 - val_accuracy: 0.4000 - val_loss: 0.6982\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4857 - loss: 0.6956 - val_accuracy: 0.4000 - val_loss: 0.6984\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.5857 - loss: 0.6936 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5571 - loss: 0.6882 - val_accuracy: 0.4000 - val_loss: 0.6986\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4857 - loss: 0.6949 - val_accuracy: 0.4000 - val_loss: 0.6987\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5143 - loss: 0.6973 - val_accuracy: 0.4000 - val_loss: 0.6986\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5000 - loss: 0.6978 - val_accuracy: 0.4000 - val_loss: 0.6984\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5286 - loss: 0.6931 - val_accuracy: 0.4000 - val_loss: 0.6981\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5143 - loss: 0.6932 - val_accuracy: 0.4000 - val_loss: 0.6977\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6916 - val_accuracy: 0.4000 - val_loss: 0.6972\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5143 - loss: 0.6909 - val_accuracy: 0.4000 - val_loss: 0.6965\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5000 - loss: 0.6909 - val_accuracy: 0.4000 - val_loss: 0.6959\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4143 - loss: 0.6995 - val_accuracy: 0.4000 - val_loss: 0.6954\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4571 - loss: 0.6955 - val_accuracy: 0.4000 - val_loss: 0.6951\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6969 - val_accuracy: 0.4000 - val_loss: 0.6949\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5286 - loss: 0.6947 - val_accuracy: 0.4000 - val_loss: 0.6947\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.4000 - val_loss: 0.6948\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4286 - loss: 0.6981 - val_accuracy: 0.4000 - val_loss: 0.6948\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4429 - loss: 0.6981 - val_accuracy: 0.4000 - val_loss: 0.6951\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5286 - loss: 0.6924 - val_accuracy: 0.4000 - val_loss: 0.6955\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.6930 - val_accuracy: 0.4000 - val_loss: 0.6960\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4286 - loss: 0.6930 - val_accuracy: 0.4000 - val_loss: 0.6966\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5143 - loss: 0.6938 - val_accuracy: 0.4000 - val_loss: 0.6974\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5143 - loss: 0.6964 - val_accuracy: 0.4000 - val_loss: 0.6979\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4857 - loss: 0.7004 - val_accuracy: 0.4000 - val_loss: 0.6983\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5000 - loss: 0.6946 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5286 - loss: 0.6897 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5143 - loss: 0.6878 - val_accuracy: 0.4000 - val_loss: 0.6987\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.5000 - loss: 0.6949 - val_accuracy: 0.4000 - val_loss: 0.6986\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5571 - loss: 0.6898 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5571 - loss: 0.6896 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5286 - loss: 0.6974 - val_accuracy: 0.4000 - val_loss: 0.6983\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4143 - loss: 0.6982 - val_accuracy: 0.4000 - val_loss: 0.6981\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5429 - loss: 0.6898 - val_accuracy: 0.4000 - val_loss: 0.6979\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5286 - loss: 0.6937 - val_accuracy: 0.4000 - val_loss: 0.6976\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4857 - loss: 0.6965 - val_accuracy: 0.4000 - val_loss: 0.6974\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5286 - loss: 0.6924 - val_accuracy: 0.4000 - val_loss: 0.6974\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4857 - loss: 0.6929 - val_accuracy: 0.4000 - val_loss: 0.6974\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5857 - loss: 0.6888 - val_accuracy: 0.4000 - val_loss: 0.6974\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6000 - loss: 0.6913 - val_accuracy: 0.4000 - val_loss: 0.6973\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5286 - loss: 0.6916 - val_accuracy: 0.4000 - val_loss: 0.6973\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4429 - loss: 0.6967 - val_accuracy: 0.4000 - val_loss: 0.6973\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4857 - loss: 0.6953 - val_accuracy: 0.4000 - val_loss: 0.6973\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5000 - loss: 0.6907 - val_accuracy: 0.4000 - val_loss: 0.6973\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.4000 - val_loss: 0.6974\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6941 - val_accuracy: 0.4000 - val_loss: 0.6975\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4571 - loss: 0.6947 - val_accuracy: 0.4000 - val_loss: 0.6975\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5286 - loss: 0.6876 - val_accuracy: 0.4000 - val_loss: 0.6978\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4429 - loss: 0.6935 - val_accuracy: 0.4000 - val_loss: 0.6980\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4857 - loss: 0.6923 - val_accuracy: 0.4000 - val_loss: 0.6982\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5857 - loss: 0.6896 - val_accuracy: 0.4000 - val_loss: 0.6984\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5143 - loss: 0.6947 - val_accuracy: 0.4000 - val_loss: 0.6984\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4286 - loss: 0.6934 - val_accuracy: 0.4000 - val_loss: 0.6981\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.5143 - loss: 0.6891 - val_accuracy: 0.4000 - val_loss: 0.6983\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5286 - loss: 0.6924 - val_accuracy: 0.4000 - val_loss: 0.6983\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4714 - loss: 0.6894 - val_accuracy: 0.4000 - val_loss: 0.6984\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5143 - loss: 0.6906 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4857 - loss: 0.6943 - val_accuracy: 0.4000 - val_loss: 0.6986\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5143 - loss: 0.6967 - val_accuracy: 0.4000 - val_loss: 0.6986\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5571 - loss: 0.6936 - val_accuracy: 0.4000 - val_loss: 0.6987\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5429 - loss: 0.6941 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5143 - loss: 0.6903 - val_accuracy: 0.4000 - val_loss: 0.6983\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5286 - loss: 0.6944 - val_accuracy: 0.4000 - val_loss: 0.6981\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5286 - loss: 0.6929 - val_accuracy: 0.4000 - val_loss: 0.6978\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5143 - loss: 0.6940 - val_accuracy: 0.4000 - val_loss: 0.6971\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4857 - loss: 0.6943 - val_accuracy: 0.4000 - val_loss: 0.6965\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4286 - loss: 0.6955 - val_accuracy: 0.4000 - val_loss: 0.6962\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5429 - loss: 0.6925 - val_accuracy: 0.4000 - val_loss: 0.6960\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5143 - loss: 0.6939 - val_accuracy: 0.4000 - val_loss: 0.6961\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5714 - loss: 0.6919 - val_accuracy: 0.4000 - val_loss: 0.6965\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4714 - loss: 0.6946 - val_accuracy: 0.4000 - val_loss: 0.6970\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5143 - loss: 0.6945 - val_accuracy: 0.4000 - val_loss: 0.6974\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4571 - loss: 0.6959 - val_accuracy: 0.4000 - val_loss: 0.6976\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5571 - loss: 0.6919 - val_accuracy: 0.4000 - val_loss: 0.6978\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5429 - loss: 0.6907 - val_accuracy: 0.4000 - val_loss: 0.6982\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5571 - loss: 0.6880 - val_accuracy: 0.4000 - val_loss: 0.6983\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5857 - loss: 0.6896 - val_accuracy: 0.4000 - val_loss: 0.6984\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5286 - loss: 0.6907 - val_accuracy: 0.4000 - val_loss: 0.6985\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5286 - loss: 0.6963 - val_accuracy: 0.4000 - val_loss: 0.6986\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "GRU Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.32      1.00      0.48         8\n",
      "\n",
      "    accuracy                           0.32        25\n",
      "   macro avg       0.16      0.50      0.24        25\n",
      "weighted avg       0.10      0.32      0.16        25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming X_train and X_test are already tokenized and indexed\n",
    "X_train_pad = pad_sequences(X_train, maxlen=50)\n",
    "X_test_pad = pad_sequences(X_test, maxlen=50)\n",
    "\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(input_dim=1000, output_dim=100, input_length=50))\n",
    "gru_model.add(SpatialDropout1D(0.2))\n",
    "gru_model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "gru_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "gru_model.fit(X_train_pad, y_train, epochs=100, batch_size=128, validation_split=0.3)\n",
    "\n",
    "# Evaluate the model\n",
    "gru_predictions = gru_model.predict(X_test_pad)\n",
    "gru_predictions = (gru_predictions > 0.5).astype(int)\n",
    "print(\"GRU Classification Report:\\n\", classification_report(y_test, gru_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECISION TREE, RANDOM FOREST AND GRID SEARCH MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70        11\n",
      "           1       0.77      0.71      0.74        14\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.72      0.72      0.72        25\n",
      "weighted avg       0.72      0.72      0.72        25\n",
      "\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64        11\n",
      "           1       0.71      0.71      0.71        14\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.68      0.68      0.68        25\n",
      "weighted avg       0.68      0.68      0.68        25\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Grid Search Best Score: 0.52\n",
      "Random Forest Grid Search Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64        11\n",
      "           1       0.71      0.71      0.71        14\n",
      "\n",
      "    accuracy                           0.68        25\n",
      "   macro avg       0.68      0.68      0.68        25\n",
      "weighted avg       0.68      0.68      0.68        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, dt_predictions))\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_predictions))\n",
    "\n",
    "# Random Forest with Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Grid Search Best Score:\", grid_search.best_score_)\n",
    "grid_rf_predictions = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"Random Forest Grid Search Classification Report:\\n\", classification_report(y_test, grid_rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 1s - 495ms/step - accuracy: 0.5444 - loss: 0.6924 - val_accuracy: 0.2000 - val_loss: 0.7043\n",
      "Epoch 2/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.4222 - loss: 0.6999 - val_accuracy: 0.2000 - val_loss: 0.7143\n",
      "Epoch 3/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5333 - loss: 0.6922 - val_accuracy: 0.2000 - val_loss: 0.7228\n",
      "Epoch 4/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5111 - loss: 0.6918 - val_accuracy: 0.2000 - val_loss: 0.7287\n",
      "Epoch 5/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5222 - loss: 0.6896 - val_accuracy: 0.2000 - val_loss: 0.7340\n",
      "Epoch 6/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5111 - loss: 0.6867 - val_accuracy: 0.2000 - val_loss: 0.7397\n",
      "Epoch 7/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5111 - loss: 0.6906 - val_accuracy: 0.2000 - val_loss: 0.7435\n",
      "Epoch 8/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5000 - loss: 0.6907 - val_accuracy: 0.2000 - val_loss: 0.7405\n",
      "Epoch 9/100\n",
      "2/2 - 0s - 54ms/step - accuracy: 0.5000 - loss: 0.6911 - val_accuracy: 0.2000 - val_loss: 0.7425\n",
      "Epoch 10/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5111 - loss: 0.6900 - val_accuracy: 0.2000 - val_loss: 0.7497\n",
      "Epoch 11/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.4667 - loss: 0.6943 - val_accuracy: 0.2000 - val_loss: 0.7589\n",
      "Epoch 12/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5111 - loss: 0.6933 - val_accuracy: 0.2000 - val_loss: 0.7664\n",
      "Epoch 13/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5111 - loss: 0.6967 - val_accuracy: 0.2000 - val_loss: 0.7753\n",
      "Epoch 14/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5111 - loss: 0.6925 - val_accuracy: 0.2000 - val_loss: 0.7724\n",
      "Epoch 15/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5222 - loss: 0.7005 - val_accuracy: 0.2000 - val_loss: 0.7527\n",
      "Epoch 16/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5222 - loss: 0.6934 - val_accuracy: 0.2000 - val_loss: 0.7358\n",
      "Epoch 17/100\n",
      "2/2 - 0s - 34ms/step - accuracy: 0.4778 - loss: 0.7114 - val_accuracy: 0.2000 - val_loss: 0.7146\n",
      "Epoch 18/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.4556 - loss: 0.6993 - val_accuracy: 0.2000 - val_loss: 0.6991\n",
      "Epoch 19/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.4444 - loss: 0.6972 - val_accuracy: 0.2000 - val_loss: 0.6967\n",
      "Epoch 20/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.4778 - loss: 0.6972 - val_accuracy: 0.2000 - val_loss: 0.6972\n",
      "Epoch 21/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5333 - loss: 0.6982 - val_accuracy: 0.2000 - val_loss: 0.7015\n",
      "Epoch 22/100\n",
      "2/2 - 0s - 40ms/step - accuracy: 0.5111 - loss: 0.6921 - val_accuracy: 0.2000 - val_loss: 0.7084\n",
      "Epoch 23/100\n",
      "2/2 - 0s - 50ms/step - accuracy: 0.5111 - loss: 0.6917 - val_accuracy: 0.2000 - val_loss: 0.7194\n",
      "Epoch 24/100\n",
      "2/2 - 0s - 40ms/step - accuracy: 0.5000 - loss: 0.6949 - val_accuracy: 0.2000 - val_loss: 0.7268\n",
      "Epoch 25/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.6947 - val_accuracy: 0.2000 - val_loss: 0.7314\n",
      "Epoch 26/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5333 - loss: 0.6937 - val_accuracy: 0.2000 - val_loss: 0.7400\n",
      "Epoch 27/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6930 - val_accuracy: 0.2000 - val_loss: 0.7489\n",
      "Epoch 28/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5222 - loss: 0.6968 - val_accuracy: 0.2000 - val_loss: 0.7514\n",
      "Epoch 29/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5111 - loss: 0.7019 - val_accuracy: 0.2000 - val_loss: 0.7478\n",
      "Epoch 30/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5111 - loss: 0.6917 - val_accuracy: 0.2000 - val_loss: 0.7431\n",
      "Epoch 31/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5222 - loss: 0.6932 - val_accuracy: 0.2000 - val_loss: 0.7400\n",
      "Epoch 32/100\n",
      "2/2 - 0s - 40ms/step - accuracy: 0.5111 - loss: 0.6933 - val_accuracy: 0.2000 - val_loss: 0.7376\n",
      "Epoch 33/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5222 - loss: 0.6977 - val_accuracy: 0.2000 - val_loss: 0.7354\n",
      "Epoch 34/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6924 - val_accuracy: 0.2000 - val_loss: 0.7329\n",
      "Epoch 35/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.4889 - loss: 0.6910 - val_accuracy: 0.2000 - val_loss: 0.7299\n",
      "Epoch 36/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5333 - loss: 0.6855 - val_accuracy: 0.2000 - val_loss: 0.7256\n",
      "Epoch 37/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5111 - loss: 0.6951 - val_accuracy: 0.2000 - val_loss: 0.7188\n",
      "Epoch 38/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.4889 - loss: 0.6959 - val_accuracy: 0.2000 - val_loss: 0.7090\n",
      "Epoch 39/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5000 - loss: 0.6949 - val_accuracy: 0.2000 - val_loss: 0.7033\n",
      "Epoch 40/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5000 - loss: 0.6940 - val_accuracy: 0.2000 - val_loss: 0.7036\n",
      "Epoch 41/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5667 - loss: 0.6933 - val_accuracy: 0.2000 - val_loss: 0.7054\n",
      "Epoch 42/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5111 - loss: 0.6928 - val_accuracy: 0.2000 - val_loss: 0.7110\n",
      "Epoch 43/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.4778 - loss: 0.6953 - val_accuracy: 0.2000 - val_loss: 0.7205\n",
      "Epoch 44/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5444 - loss: 0.6922 - val_accuracy: 0.2000 - val_loss: 0.7289\n",
      "Epoch 45/100\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.4889 - loss: 0.6940 - val_accuracy: 0.2000 - val_loss: 0.7361\n",
      "Epoch 46/100\n",
      "2/2 - 0s - 57ms/step - accuracy: 0.5222 - loss: 0.6953 - val_accuracy: 0.2000 - val_loss: 0.7398\n",
      "Epoch 47/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.6955 - val_accuracy: 0.2000 - val_loss: 0.7388\n",
      "Epoch 48/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5000 - loss: 0.6918 - val_accuracy: 0.2000 - val_loss: 0.7374\n",
      "Epoch 49/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5333 - loss: 0.6905 - val_accuracy: 0.2000 - val_loss: 0.7365\n",
      "Epoch 50/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6911 - val_accuracy: 0.2000 - val_loss: 0.7360\n",
      "Epoch 51/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6936 - val_accuracy: 0.2000 - val_loss: 0.7385\n",
      "Epoch 52/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5556 - loss: 0.6853 - val_accuracy: 0.2000 - val_loss: 0.7385\n",
      "Epoch 53/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5222 - loss: 0.6955 - val_accuracy: 0.2000 - val_loss: 0.7344\n",
      "Epoch 54/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.6933 - val_accuracy: 0.2000 - val_loss: 0.7315\n",
      "Epoch 55/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.4778 - loss: 0.6973 - val_accuracy: 0.2000 - val_loss: 0.7331\n",
      "Epoch 56/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5000 - loss: 0.6890 - val_accuracy: 0.2000 - val_loss: 0.7344\n",
      "Epoch 57/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.7036 - val_accuracy: 0.2000 - val_loss: 0.7365\n",
      "Epoch 58/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.6977 - val_accuracy: 0.2000 - val_loss: 0.7395\n",
      "Epoch 59/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.6969 - val_accuracy: 0.2000 - val_loss: 0.7374\n",
      "Epoch 60/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5333 - loss: 0.6958 - val_accuracy: 0.2000 - val_loss: 0.7330\n",
      "Epoch 61/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.4778 - loss: 0.7000 - val_accuracy: 0.2000 - val_loss: 0.7303\n",
      "Epoch 62/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5222 - loss: 0.6965 - val_accuracy: 0.2000 - val_loss: 0.7289\n",
      "Epoch 63/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5333 - loss: 0.6919 - val_accuracy: 0.2000 - val_loss: 0.7275\n",
      "Epoch 64/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5222 - loss: 0.6856 - val_accuracy: 0.2000 - val_loss: 0.7224\n",
      "Epoch 65/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5444 - loss: 0.6948 - val_accuracy: 0.2000 - val_loss: 0.7182\n",
      "Epoch 66/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5222 - loss: 0.6942 - val_accuracy: 0.2000 - val_loss: 0.7145\n",
      "Epoch 67/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.4222 - loss: 0.7010 - val_accuracy: 0.2000 - val_loss: 0.7134\n",
      "Epoch 68/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5778 - loss: 0.6889 - val_accuracy: 0.2000 - val_loss: 0.7139\n",
      "Epoch 69/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5333 - loss: 0.6955 - val_accuracy: 0.2000 - val_loss: 0.7177\n",
      "Epoch 70/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.4778 - loss: 0.6955 - val_accuracy: 0.2000 - val_loss: 0.7250\n",
      "Epoch 71/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5000 - loss: 0.6940 - val_accuracy: 0.2000 - val_loss: 0.7317\n",
      "Epoch 72/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6973 - val_accuracy: 0.2000 - val_loss: 0.7352\n",
      "Epoch 73/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.4889 - loss: 0.6950 - val_accuracy: 0.2000 - val_loss: 0.7354\n",
      "Epoch 74/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5222 - loss: 0.6951 - val_accuracy: 0.2000 - val_loss: 0.7326\n",
      "Epoch 75/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.4778 - loss: 0.6907 - val_accuracy: 0.2000 - val_loss: 0.7313\n",
      "Epoch 76/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6971 - val_accuracy: 0.2000 - val_loss: 0.7310\n",
      "Epoch 77/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5222 - loss: 0.6887 - val_accuracy: 0.2000 - val_loss: 0.7329\n",
      "Epoch 78/100\n",
      "2/2 - 0s - 45ms/step - accuracy: 0.5000 - loss: 0.6949 - val_accuracy: 0.2000 - val_loss: 0.7337\n",
      "Epoch 79/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5556 - loss: 0.6931 - val_accuracy: 0.2000 - val_loss: 0.7338\n",
      "Epoch 80/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5333 - loss: 0.6873 - val_accuracy: 0.2000 - val_loss: 0.7370\n",
      "Epoch 81/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5111 - loss: 0.6910 - val_accuracy: 0.2000 - val_loss: 0.7441\n",
      "Epoch 82/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.4889 - loss: 0.6973 - val_accuracy: 0.2000 - val_loss: 0.7487\n",
      "Epoch 83/100\n",
      "2/2 - 0s - 37ms/step - accuracy: 0.5000 - loss: 0.6951 - val_accuracy: 0.2000 - val_loss: 0.7505\n",
      "Epoch 84/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6999 - val_accuracy: 0.2000 - val_loss: 0.7499\n",
      "Epoch 85/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.4889 - loss: 0.6979 - val_accuracy: 0.2000 - val_loss: 0.7436\n",
      "Epoch 86/100\n",
      "2/2 - 0s - 43ms/step - accuracy: 0.5111 - loss: 0.7002 - val_accuracy: 0.2000 - val_loss: 0.7414\n",
      "Epoch 87/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5000 - loss: 0.7012 - val_accuracy: 0.2000 - val_loss: 0.7382\n",
      "Epoch 88/100\n",
      "2/2 - 0s - 41ms/step - accuracy: 0.5444 - loss: 0.6978 - val_accuracy: 0.2000 - val_loss: 0.7333\n",
      "Epoch 89/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.4889 - loss: 0.6964 - val_accuracy: 0.2000 - val_loss: 0.7307\n",
      "Epoch 90/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5111 - loss: 0.6916 - val_accuracy: 0.2000 - val_loss: 0.7313\n",
      "Epoch 91/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5222 - loss: 0.6885 - val_accuracy: 0.2000 - val_loss: 0.7351\n",
      "Epoch 92/100\n",
      "2/2 - 0s - 39ms/step - accuracy: 0.5111 - loss: 0.6949 - val_accuracy: 0.2000 - val_loss: 0.7390\n",
      "Epoch 93/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.4889 - loss: 0.6918 - val_accuracy: 0.2000 - val_loss: 0.7418\n",
      "Epoch 94/100\n",
      "2/2 - 0s - 58ms/step - accuracy: 0.5111 - loss: 0.6955 - val_accuracy: 0.2000 - val_loss: 0.7438\n",
      "Epoch 95/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.6851 - val_accuracy: 0.2000 - val_loss: 0.7429\n",
      "Epoch 96/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.7001 - val_accuracy: 0.2000 - val_loss: 0.7390\n",
      "Epoch 97/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5111 - loss: 0.6960 - val_accuracy: 0.2000 - val_loss: 0.7348\n",
      "Epoch 98/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5333 - loss: 0.6928 - val_accuracy: 0.2000 - val_loss: 0.7306\n",
      "Epoch 99/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5000 - loss: 0.6952 - val_accuracy: 0.2000 - val_loss: 0.7277\n",
      "Epoch 100/100\n",
      "2/2 - 0s - 38ms/step - accuracy: 0.5222 - loss: 0.6980 - val_accuracy: 0.2000 - val_loss: 0.7265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "LSTM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.32      1.00      0.48         8\n",
      "\n",
      "    accuracy                           0.32        25\n",
      "   macro avg       0.16      0.50      0.24        25\n",
      "weighted avg       0.10      0.32      0.16        25\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAHHCAYAAADqJrG+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABEiklEQVR4nO3deVxUZf//8feAMiwqArlhiiiJouaSVi653FG45lKZaYZWmt6W5pZSmUslaqVkmbbdbmHprUkuZZprppkb5q3milouuaQYoohwfn/4c75NYMFwhhnH17PHeTyc65y5rs/Mo8lPn+u6zrEYhmEIAADAAV6uDgAAANy8SCQAAIDDSCQAAIDDSCQAAIDDSCQAAIDDSCQAAIDDSCQAAIDDSCQAAIDDSCQAAIDDSCQAJ9q/f78efPBBBQYGymKxKCkpydT+Dx8+LIvFohkzZpja782sefPmat68uavDAG4ZJBLweAcPHtSzzz6rypUry9fXVyVKlFDjxo31zjvv6NKlS04dOzY2Vjt37tQbb7yh2bNnq379+k4drzD16NFDFotFJUqUyPV73L9/vywWiywWi956661893/8+HGNGjVKycnJJkQLwFmKuDoAwJmWLl2qRx99VFarVU8++aRq1qypK1euaP369Ro6dKh27dqlDz/80CljX7p0SRs3btTLL7+s5557ziljhIWF6dKlSypatKhT+v8nRYoUUXp6uhYvXqzOnTvbnUtMTJSvr68uX77sUN/Hjx/X6NGjValSJdWpUyfP71u+fLlD4wFwDIkEPFZKSoq6dOmisLAwrVq1SuXKlbOd69evnw4cOKClS5c6bfzTp09LkkqWLOm0MSwWi3x9fZ3W/z+xWq1q3LixPvvssxyJxJw5c9SmTRstWLCgUGJJT0+Xv7+/fHx8CmU8ANcwtQGPNWHCBKWlpemTTz6xSyKui4iI0IABA2yvr169qtdee01VqlSR1WpVpUqV9NJLLykjI8PufZUqVVLbtm21fv163X333fL19VXlypU1a9Ys2zWjRo1SWFiYJGno0KGyWCyqVKmSpGtTAtf//GejRo2SxWKxa1uxYoWaNGmikiVLqlixYoqMjNRLL71kO3+jNRKrVq3Sfffdp4CAAJUsWVLt27fXnj17ch3vwIED6tGjh0qWLKnAwED17NlT6enpN/5i/6Jr1676+uuvdf78eVvb5s2btX//fnXt2jXH9b///ruGDBmiWrVqqVixYipRooRatWqlHTt22K5Zs2aNGjRoIEnq2bOnbYrk+uds3ry5atasqa1bt6pp06by9/e3fS9/XSMRGxsrX1/fHJ8/JiZGQUFBOn78eJ4/K4CcSCTgsRYvXqzKlSurUaNGebr+mWee0auvvqp69epp0qRJatasmeLj49WlS5cc1x44cECPPPKIHnjgAb399tsKCgpSjx49tGvXLklSp06dNGnSJEnS448/rtmzZyshISFf8e/atUtt27ZVRkaGxowZo7ffflsPPfSQvv/++79937fffquYmBidOnVKo0aN0qBBg7RhwwY1btxYhw8fznF9586d9ccffyg+Pl6dO3fWjBkzNHr06DzH2alTJ1ksFn3xxRe2tjlz5qhatWqqV69ejusPHTqkpKQktW3bVhMnTtTQoUO1c+dONWvWzPaXevXq1TVmzBhJUu/evTV79mzNnj1bTZs2tfVz9uxZtWrVSnXq1FFCQoJatGiRa3zvvPOOSpUqpdjYWGVlZUmSPvjgAy1fvlzvvvuuQkND8/xZAeTCADxQamqqIclo3759nq5PTk42JBnPPPOMXfuQIUMMScaqVatsbWFhYYYkY926dba2U6dOGVar1Rg8eLCtLSUlxZBkvPnmm3Z9xsbGGmFhYTliGDlypPHnn+SkSZMMScbp06dvGPf1MaZPn25rq1OnjlG6dGnj7NmztrYdO3YYXl5expNPPpljvKeeesquz44dOxohISE3HPPPnyMgIMAwDMN45JFHjPvvv98wDMPIysoyypYta4wePTrX7+Dy5ctGVlZWjs9htVqNMWPG2No2b96c47Nd16xZM0OSMW3atFzPNWvWzK7tm2++MSQZr7/+unHo0CGjWLFiRocOHf7xMwL4Z1Qk4JEuXLggSSpevHierv/qq68kSYMGDbJrHzx4sCTlWEsRFRWl++67z/a6VKlSioyM1KFDhxyO+a+ur6348ssvlZ2dnaf3nDhxQsnJyerRo4eCg4Nt7XfeeaceeOAB2+f8sz59+ti9vu+++3T27Fnbd5gXXbt21Zo1a3Ty5EmtWrVKJ0+ezHVaQ7q2rsLL69p/erKysnT27FnbtM22bdvyPKbValXPnj3zdO2DDz6oZ599VmPGjFGnTp3k6+urDz74IM9jAbgxEgl4pBIlSkiS/vjjjzxdf+TIEXl5eSkiIsKuvWzZsipZsqSOHDli116xYsUcfQQFBencuXMORpzTY489psaNG+uZZ55RmTJl1KVLF82bN+9vk4rrcUZGRuY4V716dZ05c0YXL160a//rZwkKCpKkfH2W1q1bq3jx4po7d64SExPVoEGDHN/lddnZ2Zo0aZLuuOMOWa1W3XbbbSpVqpR++uknpaam5nnM8uXL52th5VtvvaXg4GAlJydr8uTJKl26dJ7fC+DGSCTgkUqUKKHQ0FD973//y9f7/rrY8Ua8vb1zbTcMw+Exrs/fX+fn56d169bp22+/Vffu3fXTTz/pscce0wMPPJDj2oIoyGe5zmq1qlOnTpo5c6YWLlx4w2qEJI0dO1aDBg1S06ZN9emnn+qbb77RihUrVKNGjTxXXqRr309+bN++XadOnZIk7dy5M1/vBXBjJBLwWG3bttXBgwe1cePGf7w2LCxM2dnZ2r9/v137b7/9pvPnz9t2YJghKCjIbofDdX+tekiSl5eX7r//fk2cOFG7d+/WG2+8oVWrVmn16tW59n09zr179+Y49/PPP+u2225TQEBAwT7ADXTt2lXbt2/XH3/8kesC1evmz5+vFi1a6JNPPlGXLl304IMPKjo6Osd3ktekLi8uXryonj17KioqSr1799aECRO0efNm0/oHbmUkEvBYL774ogICAvTMM8/ot99+y3H+4MGDeueddyRdK81LyrGzYuLEiZKkNm3amBZXlSpVlJqaqp9++snWduLECS1cuNDuut9//z3He6/fmOmvW1KvK1eunOrUqaOZM2fa/cX8v//9T8uXL7d9Tmdo0aKFXnvtNb333nsqW7bsDa/z9vbOUe3473//q2PHjtm1XU94cku68mvYsGE6evSoZs6cqYkTJ6pSpUqKjY294fcIIO+4IRU8VpUqVTRnzhw99thjql69ut2dLTds2KD//ve/6tGjhySpdu3aio2N1Ycffqjz58+rWbNm+vHHHzVz5kx16NDhhlsLHdGlSxcNGzZMHTt2VP/+/ZWenq6pU6eqatWqdosNx4wZo3Xr1qlNmzYKCwvTqVOn9P777+v2229XkyZNbtj/m2++qVatWqlhw4Z6+umndenSJb377rsKDAzUqFGjTPscf+Xl5aVXXnnlH69r27atxowZo549e6pRo0bauXOnEhMTVblyZbvrqlSpopIlS2ratGkqXry4AgICdM899yg8PDxfca1atUrvv/++Ro4caduOOn36dDVv3lwjRozQhAkT8tUfgL9w8a4RwOn27dtn9OrVy6hUqZLh4+NjFC9e3GjcuLHx7rvvGpcvX7Zdl5mZaYwePdoIDw83ihYtalSoUMGIi4uzu8Ywrm3/bNOmTY5x/rrt8EbbPw3DMJYvX27UrFnT8PHxMSIjI41PP/00x/bPlStXGu3btzdCQ0MNHx8fIzQ01Hj88ceNffv25Rjjr1skv/32W6Nx48aGn5+fUaJECaNdu3bG7t277a65Pt5ft5dOnz7dkGSkpKTc8Ds1DPvtnzdyo+2fgwcPNsqVK2f4+fkZjRs3NjZu3Jjrts0vv/zSiIqKMooUKWL3OZs1a2bUqFEj1zH/3M+FCxeMsLAwo169ekZmZqbddQMHDjS8vLyMjRs3/u1nAPD3LIaRjxVVAAAAf8IaCQAA4DASCQAA4DASCQAA4DASCQAA4DASCQAA4DASCQAA4DASCQAA4DCPvLPl5auujgBwT0ENnnN1CIDbubT9PaeP4VfXnN9eYcSaX1QkAACAwzyyIgEAgFuxeO7/t5NIAADgbBaLqyNwGhIJAACczYMrEp77yQAAgNNRkQAAwNmY2gAAAA5jagMAACAnKhIAADgbUxsAAMBhTG0AAADkREUCAABnY2oDAAA4jKkNAACAnKhIAADgbExtAAAAh3nw1AaJBAAAzubBFQnPTZEAALjFrVu3Tu3atVNoaKgsFouSkpJyXLNnzx499NBDCgwMVEBAgBo0aKCjR4/meQwSCQAAnM3iZc6RTxcvXlTt2rU1ZcqUXM8fPHhQTZo0UbVq1bRmzRr99NNPGjFihHx9ffM8BlMbAAA4m4vWSLRq1UqtWrW64fmXX35ZrVu31oQJE2xtVapUydcYVCQAALhJZGRk6MKFC3ZHRkaGQ31lZ2dr6dKlqlq1qmJiYlS6dGndc889uU5//B0SCQAAnM3LYsoRHx+vwMBAuyM+Pt6hkE6dOqW0tDSNGzdOLVu21PLly9WxY0d16tRJa9euzXM/TG0AAOBsJk1txMW9qEGDBtm1Wa1Wh/rKzs6WJLVv314DBw6UJNWpU0cbNmzQtGnT1KxZszz1QyIBAMBNwmq1Opw4/NVtt92mIkWKKCoqyq69evXqWr9+fZ77IZEAAMDZ3PA+Ej4+PmrQoIH27t1r175v3z6FhYXluR8SCQAAnM1FuzbS0tJ04MAB2+uUlBQlJycrODhYFStW1NChQ/XYY4+padOmatGihZYtW6bFixdrzZo1eR6DRAIAAA+1ZcsWtWjRwvb6+vqK2NhYzZgxQx07dtS0adMUHx+v/v37KzIyUgsWLFCTJk3yPIbFMAzD9Mhd7PJVV0cAuKegBs+5OgTA7Vza/p7Tx/B7YLwp/VxaMcyUfsxERQIAAGfjoV0AAMBhbrjY0iyemyIBAACnoyIBAICzMbUBAAAcxtQGAABATlQkAABwNqY2AACAw5jaAAAAyImKBAAAzsbUBgAAcJgHJxKe+8kAAIDTUZEAAMDZPHixJYkEAADO5sFTGyQSAAA4mwdXJDw3RQIAAE5HRQIAAGdjagMAADiMqQ0AAICcqEgAAOBkFg+uSJBIAADgZJ6cSDC1AQAAHEZFAgAAZ/PcggSJBAAAzsbUBgAAQC6oSAAA4GSeXJEgkQAAwMlIJAAAgMM8OZFgjQQAAHAYFQkAAJzNcwsSJBIAADgbUxsAAAC5IJEAAMDJLBaLKUd+rVu3Tu3atVNoaKgsFouSkpJueG2fPn1ksViUkJCQrzFIJAAAcDJXJRIXL15U7dq1NWXKlL+9buHChfrhhx8UGhqa7zFYIwEAgIdq1aqVWrVq9bfXHDt2TM8//7y++eYbtWnTJt9jkEgAAOBkZi22zMjIUEZGhl2b1WqV1Wp1qL/s7Gx1795dQ4cOVY0aNRzqw22mNr777js98cQTatiwoY4dOyZJmj17ttavX+/iyAAAKCCLOUd8fLwCAwPtjvj4eIfDGj9+vIoUKaL+/fs73IdbJBILFixQTEyM/Pz8tH37dlu2lZqaqrFjx7o4OgAA3ENcXJxSU1Ptjri4OIf62rp1q9555x3NmDGjQBUTt0gkXn/9dU2bNk0fffSRihYtamtv3Lixtm3b5sLIAAAoOLMWW1qtVpUoUcLucHRa47vvvtOpU6dUsWJFFSlSREWKFNGRI0c0ePBgVapUKc/9uMUaib1796pp06Y52gMDA3X+/PnCDwgAABO54w2punfvrujoaLu2mJgYde/eXT179sxzP26RSJQtW1YHDhzIkQGtX79elStXdk1QAACYxFWJRFpamg4cOGB7nZKSouTkZAUHB6tixYoKCQmxu75o0aIqW7asIiMj8zyGW0xt9OrVSwMGDNCmTZtksVh0/PhxJSYmasiQIerbt6+rwwMA4Ka0ZcsW1a1bV3Xr1pUkDRo0SHXr1tWrr75q2hhuUZEYPny4srOzdf/99ys9PV1NmzaV1WrVkCFD9Pzzz7s6PAAACsZFMxvNmzeXYRh5vv7w4cP5HsMtEgmLxaKXX35ZQ4cO1YEDB5SWlqaoqCgVK1bM1aEBAFBg7rhGwixuMbXx6aefKj09XT4+PoqKitLdd99NEgEAwE3ALRKJgQMHqnTp0uratau++uorZWVluTokAABM46pnbRQGt0gkTpw4oc8//1wWi0WdO3dWuXLl1K9fP23YsMHVoQEAUGAkEk5WpEgRtW3bVomJiTp16pQmTZqkw4cPq0WLFqpSpYqrwwMAADfgFost/8zf318xMTE6d+6cjhw5oj179rg6JAAACsRdqwlmcIuKhCSlp6crMTFRrVu3Vvny5ZWQkKCOHTtq165drg4NAICCMemhXe7ILSoSXbp00ZIlS+Tv76/OnTtrxIgRatiwoavDAgAA/8AtEglvb2/NmzdPMTEx8vb2dnU4AACYypOnNtwikUhMTHR1CAAAOA2JhBNMnjxZvXv3lq+vryZPnvy31/bv37+QogIAwHyenEhYjPzchNtE4eHh2rJli0JCQhQeHn7D6ywWiw4dOpSvvi9fLWh0gGcKavCcq0MA3M6l7e85fYwK/b40pZ9fprQ3pR8zuawikZKSkuufAQDwOJ5bkHCP7Z9jxoxRenp6jvZLly5pzJgxLogIAADzcGdLJxs9erTS0tJytKenp2v06NEuiAgAAOSFW+zaMAwj10xrx44dCg4OdkFEMMPncxI1c/onOnPmtKpGVtPwl0ao1p13ujosoNA0rldFA5+MVr2oiipXKlCdB36oxWt+sp2/0dz8S5MWatKslYUVJgqBu1YTzODSRCIoKMhWrqlatardF52VlaW0tDT16dPHhRHCUcu+/kpvTYjXKyNHq1at2kqcPVN9n31aXy5ZppCQEFeHBxSKAD+rdu47pllfbtTcib1znK8UHWf3+sHGNTRtZFctXJlcSBGisJBIOElCQoIMw9BTTz2l0aNHKzAw0HbOx8dHlSpV4g6XN6nZM6er0yOd1aHjw5KkV0aO1rp1a5T0xQI93Svnf1ABT7T8+91a/v3uG57/7ewfdq/bNa+ltZv36/Cxs84ODTCNSxOJ2NhYSde2gjZq1EhFixZ1ZTgwSeaVK9qze5ee7vWsrc3Ly0v33ttIP+3Y7sLIAPdVOri4WjapqV6vznZ1KHACKhJO1qxZM9ufL1++rCtXrtidL1GiRGGHhAI4d/6csrKyckxhhISEKCUlf/cEAW4VT7S7R3+kX1bSqmRXhwJn8Nw8wj0SifT0dL344ouaN2+ezp7NWdLLysq64XszMjKUkZFh12Z4W2W1Wk2PEwCc5cn292ru11uUcYU76uHm4hbbP4cOHapVq1Zp6tSpslqt+vjjjzV69GiFhoZq1qxZf/ve+Ph4BQYG2h1vjo8vpMiRm6CSQfL29s6RFJ49e1a33Xabi6IC3FfjulUUGV5W0xducHUocBLuI+Fkixcv1vvvv6+HH35YRYoU0X333adXXnlFY8eO/ccHesXFxSk1NdXuGDos7m/fA+cq6uOj6lE1tOmHjba27Oxsbdq0UXfWruvCyAD3FNuhobbuPqqd+465OhQ4iScnEm4xtfH777+rcuXKkq6th/j9998lSU2aNFHfvn3/9r1Wa85pDJ614XrdY3tqxEvDVKNGTdWsdac+nT1Tly5dUoeOnVwdGlBoAvx8VKVCKdvrSuVDdGfV8jp3IV2/nDwnSSoe4KtOD9TV8IkLXRUmCoGb5gCmcItEonLlykpJSVHFihVVrVo1zZs3T3fffbcWL16skiVLujo8OKBlq9Y69/vvev+9yTpz5rQiq1XX+x98rBCmNnALqRcVpuUfD7C9njDk2nbo2Yt+UO+Rn0qSHo25SxZZNG/ZFpfECBSUy57++WeTJk2St7e3+vfvr2+//Vbt2rWTYRjKzMzUxIkTNWDAgH/u5E+oSAC54+mfQE6F8fTPO4YuM6Wf/W+2NKUfM7lFRWLgwIG2P0dHR+vnn3/W1q1bFRERoTu5pTIA4CbH1EYhCwsLU1hYmKvDAAAA/8AtEonJkyfn2m6xWOTr66uIiAg1bdpU3t7ehRwZAAAF5647LszgFonEpEmTdPr0aaWnpysoKEiSdO7cOfn7+6tYsWI6deqUKleurNWrV6tChQoujhYAgPzx4DzCPe4jMXbsWDVo0ED79+/X2bNndfbsWe3bt0/33HOP3nnnHR09elRly5a1W0sBAABczy0qEq+88ooWLFigKlWq2NoiIiL01ltv6eGHH9ahQ4c0YcIEPfzwwy6MEgAAx3h5eW5Jwi0SiRMnTujq1Zx7Nq9evaqTJ09KkkJDQ/XHH3/kuAYAAHfH1IaTtWjRQs8++6y2b/+/R0xv375dffv21b/+9S9J0s6dOxUeHu6qEAEAuOmsW7dO7dq1U2hoqCwWi5KSkmznMjMzNWzYMNWqVUsBAQEKDQ3Vk08+qePHj+drDLdIJD755BMFBwfrrrvust3yun79+goODtYnn3wiSSpWrJjefvttF0cKAED+uepZGxcvXlTt2rU1ZcqUHOfS09O1bds2jRgxQtu2bdMXX3yhvXv36qGHHsrfZ3OHO1te9/PPP2vfvn2SpMjISEVGRjrUD3e2BHLHnS2BnArjzpa1RqwwpZ+drz3g8HstFosWLlyoDh063PCazZs36+6779aRI0dUsWLFPPXrFmskrqtcubIsFouqVKmiIkXcKjQAABxm1n0kMjIylJGRYdeW28MrHZWamiqLxZKv51y5xdRGenq6nn76afn7+6tGjRo6evSoJOn555/XuHHjXBwdAADuIT4+XoGBgXZHfHy8KX1fvnxZw4YN0+OPP64SJUrk+X1ukUjExcVpx44dWrNmjXx9fW3t0dHRmjt3rgsjAwCg4MxaIxEXF6fU1FS7Iy4ursDxZWZmqnPnzjIMQ1OnTs3Xe91i/iApKUlz587Vvffea1f+qVGjhg4ePOjCyAAAKDiztn+aOY1x3fUk4siRI1q1alW+qhGSmyQSp0+fVunSpXO0X7x40aPvTw4AgCtdTyL279+v1atXKyQkJN99uMXURv369bV06VLb6+vJw8cff6yGDRu6KiwAAEzhqu2faWlpSk5OVnJysiQpJSVFycnJOnr0qDIzM/XII49oy5YtSkxMVFZWlk6ePKmTJ0/qypUreR7DLSoSY8eOVatWrbR7925dvXpV77zzjnbv3q0NGzZo7dq1rg4PAIACcVVxfcuWLWrRooXt9aBBgyRJsbGxGjVqlBYtWiRJqlOnjt37Vq9erebNm+dpDLdIJJo0aaLk5GSNGzdOtWrV0vLly1WvXj1t3LhRtWrVcnV4AADclJo3b66/u12UGbeScotEQpKqVKmijz76yNVhAABgOk9e7+fSRMLLy+sfv1yLxZLrA70AALhZeHAe4dpEYuHChTc8t3HjRk2ePFnZ2dmFGBEAAMgPlyYS7du3z9G2d+9eDR8+XIsXL1a3bt00ZswYF0QGAIB5PHlqwy22f0rS8ePH1atXL9WqVUtXr15VcnKyZs6cqbCwMFeHBgBAgVgs5hzuyOWJRGpqqoYNG6aIiAjt2rVLK1eu1OLFi1WzZk1XhwYAgClcdR+JwuDSqY0JEyZo/PjxKlu2rD777LNcpzoAAID7cmkiMXz4cPn5+SkiIkIzZ87UzJkzc73uiy++KOTIAAAwj5sWE0zh0kTiySefdNtSDQAAZvHkv+tcmkjMmDHDlcMDAIACcps7WwIA4Kk8uCBBIgEAgLN58tSGy7d/AgCAmxcVCQAAnMyDCxIkEgAAOBtTGwAAALmgIgEAgJN5ckWCRAIAACfz4DyCRAIAAGfz5IoEayQAAIDDqEgAAOBkHlyQIJEAAMDZmNoAAADIBRUJAACczIMLEiQSAAA4m5cHZxJMbQAAAIdRkQAAwMk8uCBBIgEAgLN58q4NEgkAAJzMy3PzCNZIAAAAx1GRAADAyZjaAAAADvPgPIKpDQAAPNW6devUrl07hYaGymKxKCkpye68YRh69dVXVa5cOfn5+Sk6Olr79+/P1xgkEgAAOJnFpH/y6+LFi6pdu7amTJmS6/kJEyZo8uTJmjZtmjZt2qSAgADFxMTo8uXLeR6DqQ0AAJzMVbs2WrVqpVatWuV6zjAMJSQk6JVXXlH79u0lSbNmzVKZMmWUlJSkLl265GkMKhIAANwkMjIydOHCBbsjIyPDob5SUlJ08uRJRUdH29oCAwN1zz33aOPGjXnuh0QCAAAns1gsphzx8fEKDAy0O+Lj4x2K6eTJk5KkMmXK2LWXKVPGdi4vmNoAAMDJzNq1ERcXp0GDBtm1Wa1Wczp3EIkEAAA3CavValriULZsWUnSb7/9pnLlytnaf/vtN9WpUyfP/TC1AQCAk3lZLKYcZgoPD1fZsmW1cuVKW9uFCxe0adMmNWzYMM/9UJEAAMDJXHVDqrS0NB04cMD2OiUlRcnJyQoODlbFihX1wgsv6PXXX9cdd9yh8PBwjRgxQqGhoerQoUOexyCRAADAyVx1i+wtW7aoRYsWttfX11fExsZqxowZevHFF3Xx4kX17t1b58+fV5MmTbRs2TL5+vrmeQyLYRiG6ZG72OWrro4AcE9BDZ5zdQiA27m0/T2nj/HI9G2m9DO/Zz1T+jETFQkAAJzMk5+1QSIBAICTmb1Q0p2wawMAADiMigQAAE7mufUIEgkAAJzOVbs2CgNTGwAAwGFUJAAAcDJXPUa8MOQpkVi0aFGeO3zooYccDgYAAE/kyVMbeUok8nqrTIvFoqysrILEAwAAbiJ5SiSys7OdHQcAAB7LgwsSrJEAAMDZbvmpjb+6ePGi1q5dq6NHj+rKlSt25/r3729KYAAAeIpbfrHln23fvl2tW7dWenq6Ll68qODgYJ05c0b+/v4qXbo0iQQAALeQfN9HYuDAgWrXrp3OnTsnPz8//fDDDzpy5IjuuusuvfXWW86IEQCAm5rFYjHlcEf5TiSSk5M1ePBgeXl5ydvbWxkZGapQoYImTJigl156yRkxAgBwU7OYdLijfCcSRYsWlZfXtbeVLl1aR48elSQFBgbql19+MTc6AADg1vK9RqJu3bravHmz7rjjDjVr1kyvvvqqzpw5o9mzZ6tmzZrOiBEAgJsajxH/k7Fjx6pcuXKSpDfeeENBQUHq27evTp8+rQ8//ND0AAEAuNlZLOYc7ijfFYn69evb/ly6dGktW7bM1IAAAMDNgxtSAQDgZO6648IM+U4kwsPD//YLOXToUIECAgDA03hwHpH/ROKFF16we52Zmant27dr2bJlGjp0qFlxAQCAm0C+E4kBAwbk2j5lyhRt2bKlwAEBAOBp2LWRB61atdKCBQvM6g4AAI/Bro08mD9/voKDg83qDgAAj8Fiyz+pW7eu3RdiGIZOnjyp06dP6/333zc1OAAA4N7ynUi0b9/eLpHw8vJSqVKl1Lx5c1WrVs3U4ACYa8AbPJ0XcAXT1hG4oXwnEqNGjXJCGAAAeC5PntrId5Lk7e2tU6dO5Wg/e/asvL29TQkKAADcHPJdkTAMI9f2jIwM+fj4FDggAAA8jZfnFiTynkhMnjxZ0rXyzMcff6xixYrZzmVlZWndunWskQAAIBckEpImTZok6VpFYtq0aXbTGD4+PqpUqZKmTZtmfoQAAMBt5TmRSElJkSS1aNFCX3zxhYKCgpwWFAAAnoTFln+yevVqkggAAPLBy2LOkR9ZWVkaMWKEwsPD5efnpypVqui111674VpHhz9bft/w8MMPa/z48TnaJ0yYoEcffdSUoAAAQMGMHz9eU6dO1Xvvvac9e/Zo/PjxmjBhgt59911Tx8l3IrFu3Tq1bt06R3urVq20bt06U4ICAMCTuOJZGxs2bFD79u3Vpk0bVapUSY888ogefPBB/fjjj6Z+tnwnEmlpablu8yxatKguXLhgSlAAAHgSL4vFlCMjI0MXLlywOzIyMnIds1GjRlq5cqX27dsnSdqxY4fWr1+vVq1amfvZ8vuGWrVqae7cuTnaP//8c0VFRZkSFAAAnsTLpCM+Pl6BgYF2R3x8fK5jDh8+XF26dFG1atVUtGhR1a1bVy+88IK6detm6mfL9w2pRowYoU6dOungwYP617/+JUlauXKl5syZo/nz55saHAAA+D9xcXEaNGiQXZvVas312nnz5ikxMVFz5sxRjRo1lJycrBdeeEGhoaGKjY01LaZ8JxLt2rVTUlKSxo4dq/nz58vPz0+1a9fWqlWreIw4AAC5MGv3p9VqvWHi8FdDhw61VSWkazMKR44cUXx8vGsTCUlq06aN2rRpI0m6cOGCPvvsMw0ZMkRbt25VVlaWacEBAOAJvFxwH4n09HR5edmvYPD29lZ2drap4ziUSEjXdm988sknWrBggUJDQ9WpUydNmTLFzNgAAICD2rVrpzfeeEMVK1ZUjRo1tH37dk2cOFFPPfWUqePkK5E4efKkZsyYoU8++UQXLlxQ586dlZGRoaSkJBZaAgBwA664seW7776rESNG6N///rdOnTql0NBQPfvss3r11VdNHSfPiUS7du20bt06tWnTRgkJCWrZsqW8vb15vgYAAP/AFQ/tKl68uBISEpSQkODUcfKcSHz99dfq37+/+vbtqzvuuMOZMQEAgJtEnu8jsX79ev3xxx+66667dM899+i9997TmTNnnBkbAAAewawbUrmjPCcS9957rz766COdOHFCzz77rD7//HOFhoYqOztbK1as0B9//OHMOAEAuGm54hbZhSXfd7YMCAjQU089pfXr12vnzp0aPHiwxo0bp9KlS+uhhx5yRowAAMBN5TuR+LPIyEhNmDBBv/76qz777DOzYgIAwKO44jHihcXh+0j8mbe3tzp06KAOHTqY0R0AAB7FIjfNAkxgSiIBAABuzF2rCWYo0NQGAAC4tVGRAADAyTy5IkEiAQCAk1ncde+mCZjaAAAADqMiAQCAkzG1AQAAHObBMxtMbQAAAMdRkQAAwMnc9YFbZiCRAADAyTx5jQRTGwAAwGFUJAAAcDIPntkgkQAAwNm8eGgXAABwlCdXJFgjAQAAHEZFAgAAJ/PkXRskEgAAOJkn30eCqQ0AAOAwKhIAADiZBxckSCQAAHA2pjYAAAByQUUCAAAn8+CCBIkEAADO5snlf0/+bAAAwMmoSAAA4GQWD57bIJEAAMDJPDeNYGoDAACn87JYTDny69ixY3riiScUEhIiPz8/1apVS1u2bDH1s1GRAADAA507d06NGzdWixYt9PXXX6tUqVLav3+/goKCTB2HRAIAACdzxdTG+PHjVaFCBU2fPt3WFh4ebvo4TG0AAOBkFos5R0ZGhi5cuGB3ZGRk5DrmokWLVL9+fT366KMqXbq06tatq48++sj0z0YiAQDATSI+Pl6BgYF2R3x8fK7XHjp0SFOnTtUdd9yhb775Rn379lX//v01c+ZMU2OyGIZhmNqjG7h81dURAO5p1PJ9rg4BcDvjWld1+hifbT9mSj+dom7LUYGwWq2yWq05rvXx8VH9+vW1YcMGW1v//v21efNmbdy40ZR4JNZIAADgdGaV/2+UNOSmXLlyioqKsmurXr26FixYYFI01zC1AQCAB2rcuLH27t1r17Zv3z6FhYWZOg4VCQAAnMwVd7YcOHCgGjVqpLFjx6pz58768ccf9eGHH+rDDz80dRwqEgAAOJnFpCM/GjRooIULF+qzzz5TzZo19dprrykhIUHdunUz4yPZUJEAAMBDtW3bVm3btnXqGCQSAAA4GQ/tAgAADvPkdQQkEgAAOJknVyQ8OUkCAABORkUCAAAn89x6BIkEAABO58EzG0xtAAAAx1GRAADAybw8eHLDbSoS3333nZ544gk1bNhQx45de0ra7NmztX79ehdHBgBAwVgs5hzuyC0SiQULFigmJkZ+fn7avn277RGpqampGjt2rIujAwAAN+IWicTrr7+uadOm6aOPPlLRokVt7Y0bN9a2bdtcGBkAAAVnMekfd+QWayT27t2rpk2b5mgPDAzU+fPnCz8gAABM5K7TEmZwi4pE2bJldeDAgRzt69evV+XKlV0QEQAAyAu3SCR69eqlAQMGaNOmTbJYLDp+/LgSExM1ZMgQ9e3b19XhAQBQIF6ymHK4I7eY2hg+fLiys7N1//33Kz09XU2bNpXVatWQIUP0/PPPuzo8AAAKxJOnNtwikbBYLHr55Zc1dOhQHThwQGlpaYqKilKxYsVcHRoAAAXmyYmEW0xtfPrpp0pPT5ePj4+ioqJ09913k0QAAHATcItEYuDAgSpdurS6du2qr776SllZWa4OCQAA03jy9k+3SCROnDihzz//XBaLRZ07d1a5cuXUr18/bdiwwdWhAQBQYF4Wcw535BaJRJEiRdS2bVslJibq1KlTmjRpkg4fPqwWLVqoSpUqrg4PAADcgFsstvwzf39/xcTE6Ny5czpy5Ij27Nnj6pAAACgQd52WMINbVCQkKT09XYmJiWrdurXKly+vhIQEdezYUbt27XJ1aAAAFIgnP7TLLSoSXbp00ZIlS+Tv76/OnTtrxIgRatiwoavDAgAA/8AtEglvb2/NmzdPMTEx8vb2dnU4AACYypOnNtwikUhMTHR1CAAAOI277rgwg8sSicmTJ6t3797y9fXV5MmT//ba/v37F1JUAAAgPyyGYRiuGDg8PFxbtmxRSEiIwsPDb3idxWLRoUOH8tX35asFjQ5m+HxOomZO/0RnzpxW1chqGv7SCNW6805Xh3VLG7V8n6tDuGUZ2Vnas+wz/bJ1tS7/cV5+JYJV8e77FfnAY7K46yq6W8S41lWdPsZ3+86Z0s99VYNM6cdMLqtIpKSk5PpneIZlX3+ltybE65WRo1WrVm0lzp6pvs8+rS+XLFNISIirwwMK3b6VC5Sy4Svd9fhAFS9XUeePHtC2z99RUV9/VWn6kKvDg5N5cq7oFts/x4wZo/T09Bztly5d0pgxY1wQEQpq9szp6vRIZ3Xo+LCqRETolZGj5evrq6QvFrg6NMAlzh7eo3I171XZGg0UEFxG5es0VunIOjp3dL+rQ0MhsJh0uCO3SCRGjx6ttLS0HO3p6ekaPXq0CyJCQWReuaI9u3fp3oaNbG1eXl66995G+mnHdhdGBrhOSKXqOr1vh/44dUySlHosRWcP7VGZ6ne5ODKgYNxi14ZhGLnOEe7YsUPBwcF/+96MjAxlZGTY9+dtldVqNTVG5N258+eUlZWVYwojJCREKSn5W+8CeIqq9z+izMvp+nZcX1ksXjKMbEW17q4KdzV3dWgoBF4ePLfh0kQiKChIFotFFotFVatWtUsmsrKylJaWpj59+vxtH/Hx8TmqFi+PGKlXXh3ljJABwCHHktfr121r1eCJISpetqJSjx3ST0kfy7dEsMLuvt/V4cHJPDeNcHEikZCQIMMw9NRTT2n06NEKDAy0nfPx8VGlSpX+8Q6XcXFxGjRokF2b4U01wpWCSgbJ29tbZ8+etWs/e/asbrvtNhdFBbjW/xZPV9X7H9Ht9ZpKkgJDKyn93GntW/lfEgkUinHjxikuLk4DBgxQQkKCaf26NJGIjY2VdG0raKNGjVS0aNF892G15pzGYPunaxX18VH1qBra9MNG/ev+aElSdna2Nm3aqC6PP+Hi6ADXuHolI8fSfYuXl1y0Ax+FzcUlic2bN+uDDz7QnU7Ygu+yxZYXLlyw/blu3bq6dOmSLly4kOuBm0/32J76Yv48LUpaqEMHD+r1MaN06dIldejYydWhAS5RrkYD7V0xTyd3bdbF33/T8Z826sCaJIXW4rlCtwKLSf84Ii0tTd26ddNHH32koCDz70PhsopEUFCQTpw4odKlS6tkyZK5Lra8vggzKyvLBRGiIFq2aq1zv/+u99+brDNnTiuyWnW9/8HHCmFqA7eoOzs9qz1fJyp5wVRlpKXKr0Swwhu1VLUHu7g6NHi4fv36qU2bNoqOjtbrr79uev8uSyRWrVpl25GxevVqV4UBJ3q82xN6vBtTGYAkFfX1150de+nOjr1cHQpcwKxNG7ntVMxtiv+6zz//XNu2bdPmzZvNCSAXLkskmjVrluufAQDwNGYtkchtp+LIkSM1atSoHNf+8ssvGjBggFasWCFfX1+TIsjJZc/a+LNly5apWLFiatKkiSRpypQp+uijjxQVFaUpU6bke06HxZZA7njWBpBTYTxrY/OhVFP6ubO8b54rEklJSerYsaO8vb1tbVlZWbJYLPLy8lJGRobdOUe5xZ0thw4daltUuXPnTg0aNEitW7dWSkpKjq2dAADcdEy6R7bValWJEiXsjhtNa9x///3auXOnkpOTbUf9+vXVrVs3JScnm5JESG5yZ8uUlBRFRUVJkhYsWKB27dpp7Nix2rZtm1q3bu3i6AAAKBhHd1wURPHixVWzZk27toCAAIWEhORoLwi3qEj4+PjYHtr17bff6sEHH5QkBQcHs/0TAHDTs1jMOdyRW1QkmjRpokGDBqlx48b68ccfNXfuXEnSvn37dPvtt7s4OgAAPMOaNWtM79MtKhLvvfeeihQpovnz52vq1KkqX768JOnrr79Wy5YtXRwdAAAF48mPEXeLikTFihW1ZMmSHO2TJk1yQTQAAJjMXbMAE7hFIiFd25KSlJSkPXv2SJJq1Kihhx56yLRVpQAAwHxukUgcOHBArVu31rFjxxQZGSnp2k03KlSooKVLl6pKlSoujhAAAMe5YtdGYXGLNRL9+/dXlSpV9Msvv2jbtm3atm2bjh49qvDwcPXv39/V4QEAUCDs2nCytWvX6ocffrA9e0OSQkJCNG7cODVu3NiFkQEAgL/jFomE1WrVH3/8kaM9LS1NPj4+LogIAADzuGkxwRRuMbXRtm1b9e7dW5s2bZJhGDIMQz/88IP69Omjhx56yNXhAQBQMB68/9MtEonJkycrIiJCjRo1kq+vr3x9fdW4cWNFRETonXfecXV4AADgBlw6tZGdna0333xTixYt0pUrV9ShQwfFxsbKYrGoevXqioiIcGV4AACYwpN3bbg0kXjjjTc0atQoRUdHy8/PT1999ZUCAwP1n//8x5VhAQBgKnfdcWEGl05tzJo1S++//76++eYbJSUlafHixUpMTFR2drYrwwIAwFQevETCtYnE0aNH7R4THh0dLYvFouPHj7swKgAAkFcundq4evWqfH197dqKFi2qzMxMF0UEAIATuGs5wQQuTSQMw1CPHj1ktVptbZcvX1afPn0UEBBga/viiy9cER4AAKZgsaWTxMbG5mh74oknXBAJAABwhEsTienTp7tyeAAACoUn79pwi1tkAwDgyTw4j3CPO1sCAICbExUJAACczYNLEiQSAAA4mSfv2mBqAwAAOIyKBAAATsauDQAA4DAPziNIJAAAcDoPziRYIwEAABxGRQIAACfz5F0bJBIAADiZJy+2ZGoDAAA4jIoEAABO5sEFCRIJAACczoMzCaY2AACAw6hIAADgZJ68a4OKBAAATmaxmHPkR3x8vBo0aKDixYurdOnS6tChg/bu3Wv6ZyORAADAA61du1b9+vXTDz/8oBUrVigzM1MPPvigLl68aOo4TG0AAOBkrpjYWLZsmd3rGTNmqHTp0tq6dauaNm1q2jgkEgAAOJtJmURGRoYyMjLs2qxWq6xW6z++NzU1VZIUHBxsTjD/H1MbAAA4mcWkf+Lj4xUYGGh3xMfH/+P42dnZeuGFF9S4cWPVrFnT1M9GRQIAgJtEXFycBg0aZNeWl2pEv3799L///U/r1683PSYSCQAAnMysZ23kdRrjz5577jktWbJE69at0+23325OIH9CIgEAgJO5YrGlYRh6/vnntXDhQq1Zs0bh4eFOGYdEAgAAD9SvXz/NmTNHX375pYoXL66TJ09KkgIDA+Xn52faOCy2BADAyVxxQ6qpU6cqNTVVzZs3V7ly5WzH3LlzTf1sVCQAAHC6wp/cMAyjUMahIgEAABxGRQIAACcza9eGOyKRAADAyTw4j2BqAwAAOI6KBAAATsbUBgAAcJjFgyc3SCQAAHA2z80jWCMBAAAcR0UCAAAn8+CCBIkEAADO5smLLZnaAAAADqMiAQCAk7FrAwAAOM5z8wimNgAAgOOoSAAA4GQeXJAgkQAAwNnYtQEAAJALKhIAADgZuzYAAIDDmNoAAADIBYkEAABwGFMbAAA4mSdPbZBIAADgZJ682JKpDQAA4DAqEgAAOBlTGwAAwGEenEcwtQEAABxHRQIAAGfz4JIEiQQAAE7Grg0AAIBcUJEAAMDJ2LUBAAAc5sF5BFMbAAA4ncWkwwFTpkxRpUqV5Ovrq3vuuUc//vhjgT7KX5FIAADgoebOnatBgwZp5MiR2rZtm2rXrq2YmBidOnXKtDFIJAAAcDKLSf/k18SJE9WrVy/17NlTUVFRmjZtmvz9/fWf//zHtM9GIgEAgJNZLOYc+XHlyhVt3bpV0dHRtjYvLy9FR0dr48aNpn02FlsCAHCTyMjIUEZGhl2b1WqV1WrNce2ZM2eUlZWlMmXK2LWXKVNGP//8s2kxeWQi4euRn+rmk5GRofj4eMXFxeX6LzkK37jWVV0dAsRv41Zk1t9Lo16P1+jRo+3aRo4cqVGjRpkzgAMshmEYLhsdHu3ChQsKDAxUamqqSpQo4epwALfBbwOOyk9F4sqVK/L399f8+fPVoUMHW3tsbKzOnz+vL7/80pSYWCMBAMBNwmq1qkSJEnbHjapaPj4+uuuuu7Ry5UpbW3Z2tlauXKmGDRuaFhOTAAAAeKhBgwYpNjZW9evX1913362EhARdvHhRPXv2NG0MEgkAADzUY489ptOnT+vVV1/VyZMnVadOHS1btizHAsyCIJGA01itVo0cOZLFZMBf8NtAYXruuef03HPPOa1/FlsCAACHsdgSAAA4jEQCAAA4jEQCAAA4jEQCbqVSpUpKSEhwdRiAU6xZs0YWi0Xnz5//2+v4HeBmQiJxC+nRo4csFovGjRtn156UlCRLfp8GU0AzZsxQyZIlc7Rv3rxZvXv3LtRYgL+6/luxWCzy8fFRRESExowZo6tXrxao30aNGunEiRMKDAyUxO8AnoFE4hbj6+ur8ePH69y5c64OJVelSpWSv7+/q8MA1LJlS504cUL79+/X4MGDNWrUKL355psF6tPHx0dly5b9x8Sd3wFuJiQSt5jo6GiVLVtW8fHxN7xm/fr1uu++++Tn56cKFSqof//+unjxou38iRMn1KZNG/n5+Sk8PFxz5szJUYqdOHGiatWqpYCAAFWoUEH//ve/lZaWJulaebdnz55KTU21/V/f9QfO/Lmfrl276rHHHrOLLTMzU7fddptmzZol6drtXuPj4xUeHi4/Pz/Vrl1b8+fPN+Gbwq3OarWqbNmyCgsLU9++fRUdHa1Fixbp3LlzevLJJxUUFCR/f3+1atVK+/fvt73vyJEjateunYKCghQQEKAaNWroq6++kmQ/tcHvAJ6CROIW4+3trbFjx+rdd9/Vr7/+muP8wYMH1bJlSz388MP66aefNHfuXK1fv97uZiZPPvmkjh8/rjVr1mjBggX68MMPderUKbt+vLy8NHnyZO3atUszZ87UqlWr9OKLL0q6Vt5NSEhQiRIldOLECZ04cUJDhgzJEUu3bt20ePFiWwIiSd98843S09PVsWNHSVJ8fLxmzZqladOmadeuXRo4cKCeeOIJrV271pTvC7jOz89PV65cUY8ePbRlyxYtWrRIGzdulGEYat26tTIzMyVJ/fr1U0ZGhtatW6edO3dq/PjxKlasWI7++B3AYxi4ZcTGxhrt27c3DMMw7r33XuOpp54yDMMwFi5caFz/V+Hpp582evfubfe+7777zvDy8jIuXbpk7Nmzx5BkbN682XZ+//79hiRj0qRJNxz7v//9rxESEmJ7PX36dCMwMDDHdWFhYbZ+MjMzjdtuu82YNWuW7fzjjz9uPPbYY4ZhGMbly5cNf39/Y8OGDXZ9PP3008bjjz/+918G8Df+/FvJzs42VqxYYVitVqNDhw6GJOP777+3XXvmzBnDz8/PmDdvnmEYhlGrVi1j1KhRufa7evVqQ5Jx7tw5wzD4HcAzcIvsW9T48eP1r3/9K8f/Ae3YsUM//fSTEhMTbW2GYSg7O1spKSnat2+fihQponr16tnOR0REKCgoyK6fb7/9VvHx8fr555914cIFXb16VZcvX1Z6enqe536LFCmizp07KzExUd27d9fFixf15Zdf6vPPP5ckHThwQOnp6XrggQfs3nflyhXVrVs3X98H8FdLlixRsWLFlJmZqezsbHXt2lWdOnXSkiVLdM8999iuCwkJUWRkpPbs2SNJ6t+/v/r27avly5crOjpaDz/8sO68806H4+B3AHdHInGLatq0qWJiYhQXF6cePXrY2tPS0vTss8+qf//+Od5TsWJF7du37x/7Pnz4sNq2bau+ffvqjTfeUHBwsNavX6+nn35aV65cydcism7duqlZs2Y6deqUVqxYIT8/P7Vs2dIWqyQtXbpU5cuXt3sfzzBAQbVo0UJTp06Vj4+PQkNDVaRIES1atOgf3/fMM88oJiZGS5cu1fLlyxUfH6+3335bzz//vMOx8DuAOyORuIWNGzdOderUUWRkpK2tXr162r17tyIiInJ9T2RkpK5evart27frrrvuknTt/4j+vAtk69atys7O1ttvvy0vr2vLcObNm2fXj4+Pj7Kysv4xxkaNGqlChQqaO3euvv76az366KMqWrSoJCkqKkpWq1VHjx5Vs2bN8vfhgX8QEBCQ43dQvXp1Xb16VZs2bVKjRo0kSWfPntXevXsVFRVlu65ChQrq06eP+vTpo7i4OH300Ue5JhL8DuAJSCRuYbVq1VK3bt00efJkW9uwYcN077336rnnntMzzzyjgIAA7d69WytWrNB7772natWqKTo6Wr1799bUqVNVtGhRDR48WH5+frYtbREREcrMzNS7776rdu3a6fvvv9e0adPsxq5UqZLS0tK0cuVK1a5dW/7+/jesVHTt2lXTpk3Tvn37tHr1alt78eLFNWTIEA0cOFDZ2dlq0qSJUlNT9f3336tEiRKKjY11wreGW9kdd9yh9u3bq1evXvrggw9UvHhxDR8+XOXLl1f79u0lSS+88IJatWqlqlWr6ty5c1q9erWqV6+ea3/8DuARXL1IA4XnzwvIrktJSTF8fHyMP/+r8OOPPxoPPPCAUaxYMSMgIMC48847jTfeeMN2/vjx40arVq0Mq9VqhIWFGXPmzDFKly5tTJs2zXbNxIkTjXLlyhl+fn5GTEyMMWvWLLtFZoZhGH369DFCQkIMScbIkSMNw7BfZHbd7t27DUlGWFiYkZ2dbXcuOzvbSEhIMCIjI42iRYsapUqVMmJiYoy1a9cW7MvCLS2338p1v//+u9G9e3cjMDDQ9u/3vn37bOefe+45o0qVKobVajVKlSpldO/e3Thz5oxhGDkXWxoGvwPc/HiMOArs119/VYUKFfTtt9/q/vvvd3U4AIBCRCKBfFu1apXS0tJUq1YtnThxQi+++KKOHTumffv22eZtAQC3BtZIIN8yMzP10ksv6dChQypevLgaNWqkxMREkggAuAVRkQAAAA7jFtkAAMBhJBIAAMBhJBIAAMBhJBIAAMBhJBKAB+rRo4c6dOhge928eXO98MILhR7HmjVrZLFYdP78+UIfG0DhIJEAClGPHj1ksVhksVjk4+OjiIgIjRkzRlevXnXquF988YVee+21PF3LX/4A8oP7SACFrGXLlpo+fboyMjL01VdfqV+/fipatKji4uLsrrty5Yp8fHxMGTM4ONiUfgDgr6hIAIXMarWqbNmyCgsLU9++fRUdHa1FixbZpiPeeOMNhYaG2p7K+ssvv6hz584qWbKkgoOD1b59ex0+fNjWX1ZWlgYNGqSSJUsqJCREL774ov56e5i/Tm1kZGRo2LBhqlChgqxWqyIiIvTJJ5/o8OHDatGihSQpKChIFovF9pj57OxsxcfHKzw8XH5+fqpdu7bmz59vN85XX32lqlWrys/PTy1atLCLE4BnIpEAXMzPz09XrlyRJK1cuVJ79+7VihUrtGTJEmVmZiomJkbFixfXd999p++//17FihVTy5Ytbe95++23NWPGDP3nP//R+vXr9fvvv2vhwoV/O+aTTz6pzz77TJMnT9aePXv0wQcfqFixYqpQoYIWLFggSdq7d69OnDihd955R5IUHx+vWbNmadq0adq1a5cGDhyoJ554QmvXrpV0LeHp1KmT2rVrp+TkZD3zzDMaPny4s742AO7ChQ8MA245f36qZHZ2trFixQrDarUaQ4YMMWJjY40yZcoYGRkZtutnz55tREZG2j3tMSMjw/Dz8zO++eYbwzAMo1y5csaECRNs5zMzM43bb7/d7umVzZo1MwYMGGAYhmHs3bvXkGSsWLEi1xhze0Ll5cuXDX9/f2PDhg121z799NPG448/bhiGYcTFxRlRUVF254cNG5ajLwCehTUSQCFbsmSJihUrpszMTGVnZ6tr164aNWqU+vXrp1q1atmti9ixY4cOHDig4sWL2/Vx+fJlHTx4UKmpqTpx4oTuuece27kiRYqofv36OaY3rktOTpa3t7eaNWuW55gPHDig9PR0PfDAA3btV65cUd26dSVJe/bssYtDkho2bJjnMQDcnEgkgELWokULTZ06VT4+PgoNDVWRIv/3MwwICLC7Ni0tTXfddZcSExNz9FOqVCmHxvfz88v3e9LS0iRJS5cuVfny5e3OWa1Wh+IA4BlIJIBCFhAQoIiIiDxdW69ePc2dO1elS5dWiRIlcr2mXLly2rRpk5o2bSpJunr1qrZu3ap69erlen2tWrWUnZ2ttWvXKjo6Osf56xWRrKwsW1tUVJSsVquOHj16w0pG9erVtWjRIru2H3744Z8/JICbGostATfWrVs33XbbbWrfvr2+++47paSkaM2aNerfv79+/fVXSdKAAQM0btw4JSUl6eeff9a///3vv70HRKVKlRQbG6unnnpKSUlJtj7nzZsnSQoLC5PFYtGSJUt0+vRppaWlqXjx4hoyZIgGDhyomTNn6uDBg9q2bZveffddzZw5U5LUp08f7d+/X0OHDtXevXs1Z84czZgxw9lfEQAXI5EA3Ji/v7/WrVunihUrqlOnTqpevbqefvppXb582VahGDx4sLp3767Y2Fg1bNhQxYsXV8eOHf+236lTp+qRRx7Rv//9b1WrVk29evXSxYsXJUnly5fX6NGjNXz4cJUpU0bPPfecJOm1117TiBEjFB8fr+rVq6tly5ZaunSpwsPDJUkVK1bUggULlJSUpNq1a2vatGkaO3asE78dAO7AYtxoRRYAAMA/oCIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAcRiIBAAAc9v8ARpAcP9gd588AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences for LSTM model\n",
    "X_train_pad = pad_sequences(X_train, maxlen=50)  # Assuming max length is 50\n",
    "X_test_pad = pad_sequences(X_test, maxlen=50)\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Define the LSTM model with increased dropout to reduce overfitting\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=1000, output_dim=100, input_length=50))  # Adjust input_dim to your vocabulary size + 1\n",
    "lstm_model.add(SpatialDropout1D(0.3))  # Increased dropout\n",
    "lstm_model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))  # Increased dropout rates\n",
    "lstm_model.add(Dropout(0.3))  # Additional dropout layer after LSTM\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "lstm_model.fit(X_train_pad, y_train, epochs=100, batch_size=64, validation_split=0.1, class_weight=class_weight_dict, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_predictions = lstm_model.predict(X_test_pad)\n",
    "lstm_predictions = (lstm_predictions > 0.5).astype(int)\n",
    "\n",
    "# Print classification report with zero_division parameter to handle undefined metrics gracefully\n",
    "print(\"LSTM Classification Report:\\n\", classification_report(y_test, lstm_predictions, zero_division=0))\n",
    "\n",
    "# Plot confusion matrix to further diagnose the model's performance\n",
    "conf_matrix = confusion_matrix(y_test, lstm_predictions)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB  # Import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score  # Import classification metrics\n",
    "import pickle\n",
    "\n",
    "# Load\n",
    "\n",
    "\n",
    "# Load your cleaned and tokenized data\n",
    "df = pd.read_csv('/Users/matzchan/docs/Y1S2/SC15 DSAI/mini project/tokenized_tweets.csv')\n",
    "\n",
    "# Ensure all text data is string type and handle missing values\n",
    "df['text_str'] = df['tokenized_text'].fillna('').apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Initialize and fit the TF-IDF vectorizer with bi-grams and tri-grams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=1000)  # Adjust ngram_range and max_features as necessary\n",
    "\n",
    "# Handling potential np.nan values by replacing them with empty strings\n",
    "df['text_str'] = df['text_str'].fillna('')\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df['text_str'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "y = df['polarity']  # Ensure the polarity column exists and is appropriate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Save the TF-IDF vectorizer to disk using pickle for later use in model training\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "\n",
    "print(\"TF-IDF features are ready for model training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes model\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "nb_predictions = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Naive Bayes Classifier Metrics:\")\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, nb_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "logistic_regression_classifier = LogisticRegression(max_iter=1000)  # Adjust max_iter if needed\n",
    "logistic_regression_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "lr_predictions = logistic_regression_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Classifier Metrics:\")\n",
    "print(classification_report(y_test, lr_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "dt_predictions = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Decision Tree Classifier Metrics:\")\n",
    "print(classification_report(y_test, dt_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, dt_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "rf_predictions = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Classifier Metrics:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "rf_predictions = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Classifier Metrics:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = df['text_str'].fillna('') \n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Padding sequences to ensure uniform input size\n",
    "max_sequence_length = max(len(x) for x in sequences)  \n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "y = df['polarity'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Ensure data dimensions and types are correct\n",
    "print(\"Shape of train data:\", X_train.shape)\n",
    "print(\"Shape of label data:\", y_train.shape)\n",
    "\n",
    "# Adjust batch size and number of epochs\n",
    "batch_size = 8192  # Increase batch size for fewer steps per epoch\n",
    "epochs = 1\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"LSTM Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GRU model\n",
    "gru_model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "    GRU(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "gru_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "gru_loss, gru_accuracy = gru_model.evaluate(X_test, y_test)\n",
    "print(f\"GRU Model Accuracy: {gru_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "def load_model_and_vectorizer():\n",
    "    \"\"\" Load the pre-trained Naive Bayes model and the TF-IDF vectorizer. \"\"\"\n",
    "    try:\n",
    "        with open('naive_bayes_model.pkl', 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "        print(\"Model and vectorizer loaded successfully.\")\n",
    "        return model, vectorizer\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model or vectorizer file not found. Please check the file path.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        exit()\n",
    "\n",
    "def get_tweet_sentiment(tweet, model, vectorizer):\n",
    "    \"\"\" Predict the sentiment of a tweet using a pre-trained model and vectorizer. \"\"\"\n",
    "    if not tweet.strip():  # Check if the tweet is not just empty spaces\n",
    "        return \"No content to analyze\"\n",
    "    try:\n",
    "        # Preprocess the tweet with the vectorizer\n",
    "        tweet_vector = vectorizer.transform([tweet])\n",
    "        tweet_vector_dense = tweet_vector.toarray()  # Convert to dense numpy array\n",
    "\n",
    "        # Predict using the model\n",
    "        prediction = model.predict(tweet_vector_dense)\n",
    "        sentiment = 'positive' if prediction[0] == 1 else 'negative'\n",
    "        return sentiment\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the tweet: {e}\")\n",
    "        return \"Error in prediction\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function to load model and prompt user input for sentiment analysis. \"\"\"\n",
    "    model, vectorizer = load_model_and_vectorizer()\n",
    "    \n",
    "    while True:\n",
    "        tweet = input(\"Enter a tweet, or type 'exit' to quit: \")\n",
    "        if tweet.lower() == 'exit':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "        sentiment = get_tweet_sentiment(tweet, model, vectorizer)\n",
    "        print(f\"The sentiment of the tweet: '{tweet}' is {sentiment}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "def load_model_and_vectorizer():\n",
    "    \"\"\" Load the pre-trained Naive Bayes model and the TF-IDF vectorizer. \"\"\"\n",
    "    try:\n",
    "        with open('naive_bayes_model.pkl', 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "        print(\"Model and vectorizer loaded successfully.\")\n",
    "        return model, vectorizer\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model or vectorizer file not found. Please check the file path.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "def get_tweet_sentiment(tweet, model, vectorizer):\n",
    "    \"\"\" Predict the sentiment of a tweet using a pre-trained model and vectorizer. \"\"\"\n",
    "    if not tweet.strip():  # Check if the tweet is not just empty spaces\n",
    "        return \"No content to analyze\"\n",
    "    try:\n",
    "        # Preprocess the tweet with the vectorizer\n",
    "        tweet_vector = vectorizer.transform([tweet])\n",
    "        tweet_vector_dense = tweet_vector.toarray()  # Convert to dense numpy array\n",
    "\n",
    "        # Predict using the model\n",
    "        prediction = model.predict(tweet_vector_dense)\n",
    "        sentiment = 'positive' if prediction[0] == 1 else 'negative'\n",
    "        return sentiment\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the tweet: {e}\")\n",
    "        return \"Error in prediction\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function to load model and prompt user input for sentiment analysis. \"\"\"\n",
    "    model, vectorizer = load_model_and_vectorizer()\n",
    "    \n",
    "    while True:\n",
    "        tweet = input(\"Enter a tweet, or type 'exit' to quit: \")\n",
    "        if tweet.lower() == 'exit':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "        sentiment = get_tweet_sentiment(tweet, model, vectorizer)\n",
    "        print(f\"The sentiment of the tweet: '{tweet}' is {sentiment}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the tweet: 'HELLO' is negative\n",
      "The sentiment of the tweet: 'i love you' is negative\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def load_model_and_vectorizer():\n",
    "    \"\"\" Load the pre-trained Naive Bayes model and the TF-IDF vectorizer. \"\"\"\n",
    "    try:\n",
    "        with open('naive_bayes_model.pkl', 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
    "            vectorizer = pickle.load(file)\n",
    "        print(\"Model and vectorizer loaded successfully.\")\n",
    "        return model, vectorizer\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model or vectorizer file not found. Please check the file path.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function to load model and prompt user input for sentiment analysis. \"\"\"\n",
    "    model, vectorizer = load_model_and_vectorizer()\n",
    "    \n",
    "    while True:\n",
    "        tweet = input(\"Enter a tweet, or type 'exit' to quit: \")\n",
    "        if tweet.lower() == 'exit':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "        sentiment = get_tweet_sentiment(tweet, model, vectorizer)\n",
    "        print(f\"The sentiment of the tweet: '{tweet}' is {sentiment}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
